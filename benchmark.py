#!/usr/bin/env python3
"""
ç®€åŒ–çš„åŸºå‡†æµ‹è¯• - ç”¨äºç”Ÿæˆè®ºæ–‡æ•°æ®
==================================
"""

import asyncio
import json
import time
import statistics
from pathlib import Path
from datetime import datetime
import pandas as pd

from deepconf_with_behavior import create_integrated_analyzer

# å†…åµŒæµ‹è¯•æ•°æ®
test_cases = [
    {
        "prompt": "åˆ¶å®šä¸ªäººåŒ–çš„æœºå™¨å­¦ä¹ å­¦ä¹ è·¯å¾„ï¼Œè€ƒè™‘åŸºç¡€è–„å¼±ä½†å­¦ä¹ èƒ½åŠ›å¼ºçš„æƒ…å†µ",
        "profile": {
            "å§“å": "æå",
            "å¹´é¾„": 24,
            "ä¸“ä¸š": "è®¡ç®—æœºç§‘å­¦",
            "å½“å‰æŠ€èƒ½": ["PythonåŸºç¡€", "æ•°æ®ç»“æ„"],
            "ç›®æ ‡": "æˆä¸ºæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ",
            "å­¦ä¹ é£æ ¼": "å®è·µå¯¼å‘"
        },
        "domain": "education",
        "expected_confidence": 0.75
    },
    {
        "prompt": "åˆ†æè½¯ä»¶å·¥ç¨‹å¸ˆå‘æŠ€æœ¯ç®¡ç†å²—ä½è½¬å‹çš„å¯è¡Œæ€§å’Œè·¯å¾„",
        "profile": {
            "å§“å": "å¼ å¼º",
            "å¹´é¾„": 32,
            "å·¥ä½œå¹´é™": 8,
            "å½“å‰èŒä½": "é«˜çº§è½¯ä»¶å·¥ç¨‹å¸ˆ",
            "ç®¡ç†ç»éªŒ": "å›¢é˜ŸLead 2å¹´",
            "ç›®æ ‡": "æŠ€æœ¯æ€»ç›‘"
        },
        "domain": "career",
        "expected_confidence": 0.80
    },
    {
        "prompt": "ä¸ºä¹…åç¨‹åºå‘˜åˆ¶å®šå…¨é¢çš„å¥åº·æ”¹å–„è®¡åˆ’",
        "profile": {
            "å§“å": "é™ˆæ™¨",
            "å¹´é¾„": 29,
            "èŒä¸š": "è½¯ä»¶å¼€å‘å·¥ç¨‹å¸ˆ",
            "å¥åº·çŠ¶å†µ": {
                "BMI": 26.5,
                "è¿åŠ¨ä¹ æƒ¯": "å‡ ä¹ä¸è¿åŠ¨",
                "ç¡çœ è´¨é‡": "ç»å¸¸ç†¬å¤œ"
            },
            "ç›®æ ‡": "æ”¹å–„æ•´ä½“å¥åº·çŠ¶å†µ"
        },
        "domain": "lifestyle",
        "expected_confidence": 0.65
    },
    {
        "prompt": "è¯„ä¼°æŠ€æœ¯èƒŒæ™¯åˆ›ä¸šè€…è¿›å…¥SaaSå¸‚åœºçš„å•†ä¸šè®¡åˆ’å¯è¡Œæ€§", 
        "profile": {
            "å§“å": "å‘¨åˆ›",
            "å¹´é¾„": 35,
            "èƒŒæ™¯": "å‰å¤§å‚æŠ€æœ¯æ€»ç›‘",
            "äº§å“æƒ³æ³•": "é¢å‘ä¸­å°ä¼ä¸šçš„é¡¹ç›®ç®¡ç†SaaS",
            "é£é™©æ‰¿å—èƒ½åŠ›": "ä¸­ç­‰"
        },
        "domain": "business",
        "expected_confidence": 0.55
    },
    {
        "prompt": "åˆ¶å®šè®¡ç®—æœºè§†è§‰PhDå­¦ç”Ÿçš„ç ”ç©¶æ–¹å‘é€‰æ‹©å’Œè®ºæ–‡å‘è¡¨ç­–ç•¥",
        "profile": {
            "å§“å": "æç ”",
            "å¹´é¾„": 26,
            "å­¦å†": "ç¡•å£«åœ¨è¯»",
            "ç ”ç©¶å…´è¶£": ["ç›®æ ‡æ£€æµ‹", "å›¾åƒåˆ†å‰²"],
            "ç›®æ ‡": "é¡¶ä¼šè®ºæ–‡å‘è¡¨"
        },
        "domain": "research",
        "expected_confidence": 0.85
    },
    {
        "prompt": "ä¸ºå†…å‘å‹æŠ€æœ¯äººå‘˜åˆ¶å®šèŒåœºç¤¾äº¤èƒ½åŠ›æå‡æ–¹æ¡ˆ",
        "profile": {
            "å§“å": "èµµé™",
            "å¹´é¾„": 27,
            "æ€§æ ¼": "å†…å‘ï¼Œä¸å–„è¡¨è¾¾",
            "èŒä½": "åç«¯å¼€å‘å·¥ç¨‹å¸ˆ",
            "ç›®æ ‡": "æå‡èŒåœºå½±å“åŠ›"
        },
        "domain": "social",
        "expected_confidence": 0.70
    }
]

async def run_benchmark():
    """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ DeepConf-Behavior åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    analyzer = create_integrated_analyzer()
    results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“‹ æµ‹è¯• {i}/{len(test_cases)}: {test_case['domain']}")
        print(f"   ç”¨æˆ·: {test_case['profile'].get('å§“å', 'Unknown')}")
        
        start_time = time.time()
        
        try:
            result = await analyzer.integrated_analysis(
                prompt=test_case['prompt'],
                profile_data=test_case['profile']
            )
            
            execution_time = time.time() - start_time
            
            test_result = {
                'test_id': f"test_{i:03d}",
                'domain': test_case['domain'],
                'integrated_confidence': result.integrated_confidence,
                'analysis_consistency': result.analysis_consistency,
                'recommendation_score': result.recommendation_score,
                'deepconf_confidence': result.deepconf_confidence,
                'behavior_paths_count': len(result.behavior_paths) if result.behavior_paths else 0,
                'execution_time': execution_time,
                'expected_confidence': test_case['expected_confidence'],
                'confidence_error': abs(result.integrated_confidence - test_case['expected_confidence']),
                'status': 'success'
            }
            
            results.append(test_result)
            
            print(f"   âœ… å®Œæˆ - ç½®ä¿¡åº¦: {result.integrated_confidence:.3f} (é¢„æœŸ: {test_case['expected_confidence']:.3f})")
            print(f"   â±ï¸ è€—æ—¶: {execution_time:.1f}s")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {str(e)}")
            results.append({
                'test_id': f"test_{i:03d}",
                'domain': test_case['domain'],
                'error': str(e),
                'execution_time': time.time() - start_time,
                'status': 'failed'
            })
        
        # é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
        await asyncio.sleep(2)
    
    # åˆ†æç»“æœ
    successful_results = [r for r in results if r['status'] == 'success']
    
    print("\n" + "=" * 50)
    print("ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ")
    print("=" * 50)
    
    if successful_results:
        # åŸºç¡€ç»Ÿè®¡
        confidences = [r['integrated_confidence'] for r in successful_results]
        consistencies = [r['analysis_consistency'] for r in successful_results]
        exec_times = [r['execution_time'] for r in successful_results]
        errors = [r['confidence_error'] for r in successful_results]
        
        print(f"âœ… æˆåŠŸç‡: {len(successful_results)}/{len(results)} ({len(successful_results)/len(results)*100:.1f}%)")
        print(f"ğŸ“ˆ å¹³å‡ç½®ä¿¡åº¦: {statistics.mean(confidences):.3f} Â± {statistics.stdev(confidences) if len(confidences) > 1 else 0:.3f}")
        print(f"ğŸ”„ å¹³å‡ä¸€è‡´æ€§: {statistics.mean(consistencies):.3f} Â± {statistics.stdev(consistencies) if len(consistencies) > 1 else 0:.3f}")
        print(f"â±ï¸ å¹³å‡æ‰§è¡Œæ—¶é—´: {statistics.mean(exec_times):.1f}s Â± {statistics.stdev(exec_times) if len(exec_times) > 1 else 0:.1f}s")
        print(f"ğŸ¯ å¹³å‡é¢„æµ‹è¯¯å·®: {statistics.mean(errors):.3f}")
        
        # æŒ‰é¢†åŸŸç»Ÿè®¡
        domain_stats = {}
        for result in successful_results:
            domain = result['domain']
            if domain not in domain_stats:
                domain_stats[domain] = []
            domain_stats[domain].append(result)
        
        print(f"\nğŸ“ˆ æŒ‰é¢†åŸŸç»Ÿè®¡:")
        for domain, domain_results in domain_stats.items():
            domain_confidences = [r['integrated_confidence'] for r in domain_results]
            avg_conf = statistics.mean(domain_confidences)
            print(f"  {domain}: å¹³å‡ç½®ä¿¡åº¦ {avg_conf:.3f} ({len(domain_results)}ä¸ªæµ‹è¯•)")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = Path("benchmark_results")
    results_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜åŸå§‹æ•°æ®
    with open(results_dir / f"benchmark_results_{timestamp}.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    # ä¿å­˜CSVæ ¼å¼
    if results:
        df = pd.DataFrame(results)
        df.to_csv(results_dir / f"benchmark_summary_{timestamp}.csv", index=False)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
    print(f"   JSON: benchmark_results/benchmark_results_{timestamp}.json")
    print(f"   CSV: benchmark_results/benchmark_summary_{timestamp}.csv")
    
    return results

if __name__ == "__main__":
    asyncio.run(run_benchmark())